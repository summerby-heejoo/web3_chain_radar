import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import requests
import feedparser
from bs4 import BeautifulSoup
from datetime import datetime
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import random
import re
import os
from sklearn.metrics.pairwise import cosine_similarity

# KeyBERT (í‚¤ì›Œë“œ ì¶”ì¶œ) â€“ ì„¤ì¹˜ ì•ˆ ë¼ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ fallback ë˜ë„ë¡ ì²˜ë¦¬
try:
    from keybert import KeyBERT
    kw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')
except Exception:
    kw_model = None



# ===============================================
# Streamlit Basic Setup
# ===============================================
st.set_page_config(
    page_title="Web3 Chain Radar",
    page_icon="ğŸ‚",
    layout="wide"
)

st.markdown("""
<style>
[data-testid="block-container"] {
    padding-left: 2rem;
    padding-right: 2rem;
    padding-top: 1rem;
}
[data-testid="stMetric"] {
    background-color: #2b2b2b;
    padding: 14px;
    border-radius: 8px;
    text-align: center;
}
</style>
""", unsafe_allow_html=True)

# ===============================================
# ìˆ«ìí˜• Metric (ê°€ê²©/ë³€ë™ë¥ )
# ===============================================

def colored_metric(label, price, change):
    color = "green" if change >= 0 else "red"
    arrow = "â–²" if change >= 0 else "â–¼"

    st.markdown(
        f"""
        <div style='background-color:#2b2b2b;
                    padding:12px;
                    border-radius:8px;
                    text-align:center;
                    color:white;'>
            <div style='font-size:18px;'>{label}</div>
            <div style='font-size:22px; font-weight:bold;'>${price:,}</div>
            <div style='font-size:18px; color:{color};'>
                {arrow} {abs(change)}%
            </div>
        </div>
        """,
        unsafe_allow_html=True
    )

# ===============================================
# ìƒíƒœìš© Metric (ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ + í™•ì¥/ì¶•ì†Œ)
# green / yellow / red ìë™ ìƒ‰ìƒ ì ìš©
# ===============================================
def colored_status(label, value):
    # ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ ì§€ì •
    if value in ["ë†’ìŒ", "í™•ì¥ êµ­ë©´"]:
        color = "limegreen"
    elif value in ["ì¤‘ê°„"]:
        color = "gold"
    elif value in ["ë‚®ìŒ", "ì¶•ì†Œ êµ­ë©´"]:
        color = "red"
    else:
        color = "white"

    st.markdown(
        f"""
        <div style='background-color:#2b2b2b;
                    padding:12px;
                    border-radius:8px;
                    text-align:center;
                    color:white;'>
            <div style='font-size:18px;'>{label}</div>
            <div style='font-size:22px; font-weight:bold; color:{color};'>
                {value}
            </div>
        </div>
        """,
        unsafe_allow_html=True
    )

# ===============================================
# Global Market Summary ë””ìì¸
# ===============================================
def custom_metric(label, value, change=None):
    """
    - value: ìˆ«ì/ë¬¸ì ê·¸ëŒ€ë¡œ í‘œì‹œ
    - change: +% or -%
    ìƒ‰ìƒì€ ë³€ë™ë¥  ê¸°ì¤€ìœ¼ë¡œ ìë™ ê²°ì •
    """

    if change is None:
        color = "white"
        arrow = ""
        change_text = ""
    else:
        if change >= 0:
            color = "limegreen"
            arrow = "â–²"
        else:
            color = "red"
            arrow = "â–¼"

        change_text = f"<div style='font-size:18px; color:{color};'>{arrow} {abs(change):.2f}%</div>"

    st.markdown(
        f"""
        <div style='background-color:#2b2b2b;
                    padding:12px;
                    border-radius:8px;
                    text-align:center;
                    color:white;'>
            <div style='font-size:18px;'>{label}</div>
            <div style='font-size:22px; font-weight:bold;'>{value}</div>
            {change_text}
        </div>
        """,
        unsafe_allow_html=True
    )

# ===============================================
# fear&greed ë””ìì¸
# ===============================================
def fear_greed_card(score, diff):
    # ìƒ‰ìƒ ê·œì¹™
    if score >= 70:
        color = "limegreen"
    elif score >= 40:
        color = "gold"
    else:
        color = "red"

    arrow = "â–²" if diff >= 0 else "â–¼"

    st.markdown(
        f"""
        <div style='background-color:#2b2b2b;
                    padding:12px;
                    border-radius:8px;
                    text-align:center;
                    color:white;'>
            <div style='font-size:18px;'>Fear & Greed Index</div>
            <div style='font-size:22px; font-weight:bold; color:{color};'>{score}</div>
            <div style='font-size:18px; color:{color};'>
                {arrow} {abs(diff)}
            </div>
        </div>
        """,
        unsafe_allow_html=True
    )


# ===============================================
# í•œêµ­ì–´ í°íŠ¸
# ===============================================

def generate_wordcloud(text):
    # í•œê¸€ í°íŠ¸ ê²½ë¡œ (ë„ˆê°€ ì—…ë¡œë“œí•œ NanumGothic.ttf)
    font_path = "fonts/NanumGothic.ttf"
    if not os.path.exists(font_path):
        font_path = None  # í°íŠ¸ ì—†ìœ¼ë©´ fallback

    wc = WordCloud(
        width=800,
        height=400,
        background_color="black",
        font_path=font_path,
        colormap="cool"
    ).generate(text)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    return fig



# ===============================================
# Fear & Greed Proxy API (ì•ˆì •ì , ì°¨ë‹¨ ì—†ìŒ)
# ===============================================
@st.cache_data(ttl=3600)
def load_fear_greed_api():
    url = "https://api.alternative.me/fng/?limit=2&format=json"  # ìµœê·¼ 2ì¼ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

    try:
        r = requests.get(url, timeout=5)
        data = r.json()

        today = data["data"][0]          # ì˜¤ëŠ˜ ë°ì´í„°
        yesterday = data["data"][1]      # ì „ì¼ ë°ì´í„°

        now_score = int(today["value"])
        prev_score = int(yesterday["value"])
        diff = now_score - prev_score
        rating = today["value_classification"]

        # timestamp â†’ ë‚ ì§œ ë³€í™˜
        ts = int(today["timestamp"])
        today_date = datetime.fromtimestamp(ts)

        # íˆìŠ¤í† ë¦¬ìš© ë°ì´í„°í”„ë ˆì„ ìƒì„± (2ì¼ ì´ìƒ í™•ì¥í•˜ë ¤ë©´ limit=30ìœ¼ë¡œ ë°”ê¾¸ë©´ ë¨)
        hist = pd.DataFrame([
            {
                "date": datetime.fromtimestamp(int(item["timestamp"])),
                "score": int(item["value"])
            }
            for item in data["data"]
        ])

        return {
            "score": now_score,
            "rating": rating,
            "diff": diff,
            "hist": hist.sort_values("date")
        }

    except Exception as e:
        st.error(f"Fear & Greed Proxy API ì˜¤ë¥˜: {e}")
        return {
            "score": 50,
            "rating": "Neutral",
            "diff": 0,
            "hist": pd.DataFrame({
                "date": pd.date_range(end=pd.Timestamp.today(), periods=30),
                "score": np.random.randint(40, 60, 30),
            })
        }


# ===============================================
# BTC Active Addresses (ì‹¤ì‹œê°„ ë°ì´í„°)
# - ë¬´ë£Œ API: Blockchain.com Charts
# - ì˜ë¯¸: ìµœê·¼ 30ì¼ ë™ì•ˆ ì‹¤ì œ ì‚¬ìš©ëœ BTC ì£¼ì†Œ ìˆ˜
# - ìš©ë„: ë„¤íŠ¸ì›Œí¬ í™œì„±ë„ / ì‹œì¥ ê°•ë„ íŒë‹¨
# ===============================================
@st.cache_data(ttl=300)  # 5ë¶„ ìºì‹œ
def load_btc_active_addresses():
    """
    Blockchain.com Charts APIë¥¼ ì´ìš©í•˜ì—¬
    ìµœê·¼ 30ì¼ ë™ì•ˆì˜ Bitcoin í™œì„± ì£¼ì†Œ(active addresses) ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.

    ë°˜í™˜ë˜ëŠ” ë°ì´í„°:
        - date: ë‚ ì§œ(datetime)
        - active_addresses: í™œì„± ì£¼ì†Œ ìˆ˜(int)
    """
    url = "https://api.blockchain.info/charts/n-unique-addresses?timespan=30days&format=json"

    try:
        # API í˜¸ì¶œ
        r = requests.get(url, timeout=5)
        js = r.json()

        # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
        df = pd.DataFrame(js["values"])
        df["date"] = df["x"].apply(lambda t: datetime.fromtimestamp(t))
        df = df.rename(columns={"y": "active_addresses"})

        return df[["date", "active_addresses"]]

    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (ì„œë¹„ìŠ¤ ì§€ì†ì„± í™•ë³´)
        st.error(f"BTC Active Addresses API ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame({
            "date": pd.date_range(end=pd.Timestamp.today(), periods=30),
            "active_addresses": np.random.randint(700000, 900000, 30)
        })


# ===============================================
# CoinGecko ì‹¤ì‹œê°„ ê°€ê²© API
# ===============================================
@st.cache_data(ttl=60)
def load_prices_multi(coin_list):
    """
    coin_list í˜•ì‹:
    [
        {"id": "bitcoin", "symbol": "BTC"},
        {"id": "ethereum", "symbol": "ETH"},
        {"id": "solana", "symbol": "SOL"},
    ]
    """

    ids = ",".join([c["id"] for c in coin_list])

    url = (
        f"https://api.coingecko.com/api/v3/simple/price"
        f"?ids={ids}&vs_currencies=usd&include_24hr_change=true"
    )

    r = requests.get(url, timeout=5)
    data = r.json()

    output = {}
    for c in coin_list:
        cid = c["id"]
        symbol = c["symbol"]

        if cid in data:
            output[symbol] = {
                "price": data[cid]["usd"],
                "change": round(data[cid]["usd_24h_change"], 2)
            }

    return output

# ===============================================
# ê¸€ë¡œë²Œ ë§ˆì¼“ ìš”ì•½ (CoinGecko Global API)
# ===============================================
@st.cache_data(ttl=300)
def load_global_market():
    url = "https://api.coingecko.com/api/v3/global"
    try:
        r = requests.get(url, timeout=5)
        data = r.json()["data"]

        return {
            "total_mcap": data["total_market_cap"].get("usd", 0),
            "mcap_change_24h": data.get("market_cap_change_percentage_24h_usd", 0),
            "btc_dominance": data["market_cap_percentage"].get("btc", 0),
            "active_coins": data.get("active_cryptocurrencies", 0)
        }
    except Exception as e:
        st.error(f"Global Market API ì˜¤ë¥˜: {e}")
        return {
            "total_mcap": 0,
            "mcap_change_24h": 0,
            "btc_dominance": 0,
            "active_coins": 0
        }

# ===============================================
# Web3 ì„¹í„° ì‹œì´ ë°ì´í„° (ì‹¤ì‹œê°„: CoinGecko Categories API)
# ===============================================
# ===============================================
# Web3 ì„¹í„° ì‹œì´ ë°ì´í„° (ì‹¤ì‹œê°„: CoinGecko Categories API)
#  - ì›ì‹œ ì¹´í…Œê³ ë¦¬ â†’ í•µì‹¬ 6ê°œ ì„¹í„°ë¡œ ë¶„ë¥˜
# ===============================================
def _classify_core_sector(name: str) -> str:
    n = name.lower()

    if "ai" in n or "artificial intelligence" in n:
        return "AI"
    if "layer 2" in n or "layer-2" in n or "l2" in n or "rollup" in n:
        return "Layer2"
    if "defi" in n or "dex" in n or "yield" in n or "lending" in n or "amm" in n:
        return "DeFi"
    if "nft" in n or "collectible" in n:
        return "NFT"
    if "gaming" in n or "gamefi" in n or "metaverse" in n:
        return "Gaming"
    if "real world" in n or "rwa" in n or "tokenized" in n:
        return "RWA"
    return "Infra/ê¸°íƒ€"


@st.cache_data(ttl=300)
def load_sectors_realtime():
    url = "https://api.coingecko.com/api/v3/coins/categories"

    try:
        r = requests.get(url, timeout=5)
        data = r.json()

        sectors = []
        for d in data:
            name = d.get("name", "Unknown")
            mc = d.get("market_cap", 0)
            mc_chg = d.get("market_cap_change_24h", 0)
            category_id = d.get("id", "")

            sectors.append({
                "category_id": category_id,
                "sector": name,
                "market_cap": mc,
                "market_cap_change_24h": mc_chg,
                "core_sector": _classify_core_sector(name)
            })

        df = pd.DataFrame(sectors)
        return df

    except Exception as e:
        st.error(f"Sectors API ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=["category_id", "sector", "market_cap", "market_cap_change_24h", "core_sector"])



# ===============================================
# ì„¹í„°ë³„ Top ìƒìŠ¹/í•˜ë½ í”„ë¡œì íŠ¸
# category_id ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ
# ===============================================
@st.cache_data(ttl=300)
def load_sector_top_movers(category, top=10):
    url = (
        "https://api.coingecko.com/api/v3/coins/markets"
        f"?vs_currency=usd&category={category}&order=market_cap_desc"
        "&price_change_percentage=24h&per_page=100&page=1"
    )

    try:
        r = requests.get(url, timeout=5)
        data = r.json()
        df = pd.DataFrame(data)

        df = df[["name", "symbol", "current_price", "price_change_percentage_24h"]]

        top_gainers = df.sort_values("price_change_percentage_24h", ascending=False).head(top)
        top_losers = df.sort_values("price_change_percentage_24h").head(top)

        return top_gainers, top_losers

    except:
        return pd.DataFrame(), pd.DataFrame()


# ===============================================
# NEWS FETCH â€” (1) Google News (KR, crypto) + (2) Cointelegraph RSS
# ===============================================
# ===============================================
# NEWS FETCH â€” CryptoPanic + Cointelegraph + í•œêµ­ì–´ ë‰´ìŠ¤ í˜¼í•©
#  - ê¸€ë¡œë²Œ: CryptoPanic, Cointelegraph
#  - í•œêµ­ì–´: Google News(ì•”í˜¸í™”í/ë¸”ë¡ì²´ì¸ ê²€ìƒ‰)
# ===============================================
@st.cache_data(ttl=1800)
def load_news_all():

    news_items = []

    # -------- 1) CryptoPanic API (ê¸€ë¡œë²Œ, ì˜ì–´) --------
    try:
        res = requests.get("https://cryptopanic.com/api/v1/posts/?auth_token=&public=true", timeout=5)
        js = res.json()
        for item in js.get("results", []):
            news_items.append({
                "title": item["title"],
                "source": item["source"]["title"],
                "summary_raw": item.get("description", item["title"]),
                "lang": "en"
            })
    except:
        pass

    # -------- 2) Cointelegraph RSS (ê¸€ë¡œë²Œ, ì˜ì–´) --------
    try:
        feed = feedparser.parse("https://cointelegraph.com/rss")
        for entry in feed.entries[:10]:
            news_items.append({
                "title": entry.title,
                "source": "Cointelegraph",
                "summary_raw": BeautifulSoup(entry.summary, "html.parser").text,
                "lang": "en"
            })
    except:
        pass

    # -------- 3) Google News RSS (í•œêµ­ì–´, 'ì•”í˜¸í™”í OR ë¹„íŠ¸ì½”ì¸ OR ë¸”ë¡ì²´ì¸') --------
    # -------- í•œêµ­ì–´ Google News (ë³¸ë¬¸ í¬í•¨) --------
    kr_feed_url = (
        "https://news.google.com/rss/search?"
        "q=ì•”í˜¸í™”í+OR+ë¹„íŠ¸ì½”ì¸+OR+ë¸”ë¡ì²´ì¸&hl=ko&gl=KR&ceid=KR:ko"
    )
    feed_kr = feedparser.parse(kr_feed_url)

    for entry in feed_kr.entries[:40]:
        url = entry.link.replace("./articles/", "https://news.google.com/articles/")
        body = extract_article_body(url)

        news_items.append({
            "title": entry.title,
            "source": "Google News KR",
            "summary_raw": body if len(body) > 100 else entry.title,  # ë³¸ë¬¸ ìš°ì„ 
            "url": url,
            "lang": "ko"
        })

    # -------- 4) (ì˜µì…˜) ì½”ì¸ë°ìŠ¤í¬ í•œêµ­ì–´ HTML ìŠ¤í¬ë˜í•‘ â€” êµ¬ì¡° ë°”ë€Œë©´ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ --------
    try:
        r = requests.get("https://www.coindesk.com/ko", timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")
        # ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œ ê¸°ì¤€ìœ¼ë¡œ ì œëª© ì¼ë¶€ ê¸ê¸° (í•„ìš”ì‹œ ì§ì ‘ class ìˆ˜ì •í•˜ë©´ ë¨)
        for h in soup.find_all("h3")[:15]:
            title = h.get_text(strip=True)
            if not title:
                continue
            news_items.append({
                "title": title,
                "source": "ì½”ì¸ë°ìŠ¤í¬ ì½”ë¦¬ì•„(ìŠ¤í¬ë©)",
                "summary_raw": title,
                "lang": "ko"
            })
    except:
        pass

    df = pd.DataFrame(news_items)
    if df.empty:
        return pd.DataFrame(columns=["title", "source", "summary_raw", "lang"])
    return df


# ===============================================
# ìš”ì•½ í•¨ìˆ˜ (KR/EN ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥ â€“ í•µì‹¬ ë¬¸ì¥ 2~3ê°œ ì¶”ì¶œ)
# ===============================================

def summarize(text, max_sentences=3, max_chars=400):
    if not isinstance(text, str):
        return ""

    text = text.replace("\n", " ").strip()
    if len(text) == 0:
        return ""

    # ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ
    if len(text) <= 120:
        return text

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë¼ë³´ê¸° (ì˜ì–´/í•œêµ­ì–´ í˜¼í•© ê³ ë ¤)
    # . ? ! ê¸°ì¤€ split
    sentences = re.split(r'(?<=[\.\?\!])\s+', text)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]

    # ë¬¸ì¥ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì•ë¶€ë¶„ë§Œ ìë¥¸ë‹¤
    if len(sentences) <= 1:
        return (text[:max_chars] + "...") if len(text) > max_chars else text

    # ê¸¸ì´ ê¸°ì¤€ ìƒìœ„ ë¬¸ì¥ë“¤ ë½‘ì•„ì„œ ìš”ì•½ (ê°„ë‹¨í•œ heuristic)
    ranked = sorted(
        sentences,
        key=lambda s: len(s),
        reverse=True
    )

    picked = ranked[:max_sentences]
    summary = " ".join(picked)
    if len(summary) > max_chars:
        summary = summary[:max_chars] + "..."
    return summary

# ===============================================
# ë‰´ìŠ¤ ë³¸ë¬¸ ë„˜ê¸°ê¸° (ë¬¸ì¥ ë‹¨ìœ„ ê·¸ë˜í”„ ë­í‚¹)
# ===============================================

def extract_article_body(url):
    try:
        r = requests.get(url, timeout=5)
        soup = BeautifulSoup(r.text, "html.parser")

        # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê³µí†µ íŒ¨í„´
        selectors = [
            "article", 
            ".article-body",
            ".article-content",
            ".content",
            "#article",
            ".post-content"
        ]

        for sel in selectors:
            body = soup.select_one(sel)
            if body:
                text = body.get_text(" ", strip=True)
                if len(text) > 150:  # ë³¸ë¬¸ ìµœì†Œ ê¸¸ì´
                    return text

        # fallback: ë¬¸ì„œ ì „ì²´
        return " ".join([p.get_text(strip=True) for p in soup.find_all("p")])[:2000]

    except:
        return ""


# ===============================================
# í•œêµ­ì–´ TextRank ìš”ì•½ (ë¬¸ì¥ ë‹¨ìœ„ ê·¸ë˜í”„ ë­í‚¹)
# ===============================================
def textrank_summarize(text, max_sent=3):
    text = text.replace("\n", " ").strip()
    if len(text) < 40:   # ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëƒ¥ ë°˜í™˜
        return text

    # 1) ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´ë¼ ëŒ€ì¶© . ? ! ì™€ 'ë‹¤.' ê¸°ì¤€)
    #    ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ì‹¤ë¬´ìš©ìœ¼ë¡  ì¶©ë¶„
    import re
    # ë¨¼ì € ë§ˆì¹¨í‘œ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³ , ë„ˆë¬´ ì§§ì€ ì¡°ê°ì€ ë²„ë¦¼
    raw_sents = re.split(r'(?<=[\.!?])\s+', text)
    sents = [s.strip() for s in raw_sents if len(s.strip()) > 10]

    if len(sents) <= max_sent:
        return " ".join(sents)

    # 2) TF-IDFë¡œ ë¬¸ì¥ ë²¡í„°í™”
    vectorizer = TfidfVectorizer(stop_words="english")  # í•œ/ì˜ í˜¼ìš©ì´ë¼ ê·¸ëƒ¥ english stopwordë§Œ
    X = vectorizer.fit_transform(sents)

    # 3) ë¬¸ì¥ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ â†’ ê·¸ë˜í”„ (TextRank ê¸°ë³¸ êµ¬ì¡°)
    sim_matrix = cosine_similarity(X, X)

    # 4) TextRank ë°˜ë³µ (PageRank ìœ ì‚¬)
    n = sim_matrix.shape[0]
    scores = np.ones(n) / n
    d = 0.85  # damping factor

    for _ in range(20):
        scores = (1 - d) / n + d * sim_matrix.dot(scores) / (sim_matrix.sum(axis=1) + 1e-8)

    # 5) ìƒìœ„ ì ìˆ˜ ë¬¸ì¥ max_sentê°œ ì„ íƒ (ì›ë˜ ìˆœì„œ ìœ ì§€)
    ranked_idx = np.argsort(scores)[::-1][:max_sent]
    ranked_idx = sorted(ranked_idx)  # ì›ë˜ ë“±ì¥ ìˆœì„œ

    selected = [sents[i] for i in ranked_idx]
    summary = " ".join(selected)

    return summary


# ===============================================
# KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (fallback í¬í•¨)
# ===============================================
def extract_keywords(text, top_k=5):
    text = text.replace("\n", " ").strip()
    if len(text) < 20:
        return []

    # 1) KeyBERTê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ê·¸ê±¸ë¡œ
    if kw_model is not None:
        try:
            keywords = kw_model.extract_keywords(
                text,
                keyphrase_ngram_range=(1, 2),
                stop_words='english',
                top_n=top_k
            )
            return [k[0] for k in keywords]
        except Exception:
            pass

    # 2) ì‹¤íŒ¨í•˜ë©´ TF-IDF ê¸°ë°˜ ê°„ì´ í‚¤ì›Œë“œ
    vectorizer = TfidfVectorizer(
        max_features=200,
        stop_words="english"
    )
    X = vectorizer.fit_transform([text])
    scores = X.toarray()[0]
    terms = vectorizer.get_feature_names_out()

    idx = np.argsort(scores)[::-1][:top_k]
    return [terms[i] for i in idx]



# ===============================================
# Topic Clustering (ë‰´ìŠ¤ í† í”½ í´ëŸ¬ìŠ¤í„°ë§)
# ===============================================
def topic_clustering(df, n_clusters=5):
    if df.empty:
        df["topic"] = []
        return df

    texts = df["summary_raw"].fillna("").tolist()
    if len(texts) < 3:
        df["topic"] = 0
        return df

    vectorizer = TfidfVectorizer(stop_words="english", max_features=500)
    X = vectorizer.fit_transform(texts)

    # ë°ì´í„° ìˆ˜ë³´ë‹¤ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë§ì§€ ì•Šë„ë¡ ì¡°ì •
    k = min(n_clusters, max(1, len(df) // 3))
    model = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = model.fit_predict(X)

    df["topic"] = labels
    return df


# ===============================================
# Global Market Summary (ì‹œì´ / ë„ë¯¸ë„ŒìŠ¤ / ê±°ë˜ëŸ‰)
# CoinGecko Free API
# ===============================================
@st.cache_data(ttl=300)
def load_global_market():
    url = "https://api.coingecko.com/api/v3/global"

    try:
        r = requests.get(url, timeout=5)
        data = r.json()["data"]

        return {
            "market_cap": data["total_market_cap"]["usd"],
            "volume_24h": data["total_volume"]["usd"],
            "btc_dominance": data["market_cap_percentage"]["btc"],
            "eth_dominance": data["market_cap_percentage"]["eth"],
            "market_cap_change_24h": data.get("market_cap_change_percentage_24h_usd", 0)
        }

    except Exception as e:
        st.error(f"Global Market API ì˜¤ë¥˜: {e}")
        return {
            "market_cap": 0,
            "volume_24h": 0,
            "btc_dominance": 0,
            "eth_dominance": 0,
            "market_cap_change_24h": 0
        }


# ===============================================
# Navigation
# ===============================================
page = st.sidebar.radio(
    "Navigation",
    ["ğŸ“Œ Home", "ğŸ“° News", "ğŸ§© Sectors"]
)


# ===============================================
# PAGE 1 â€” HOME
# ===============================================
if page == "ğŸ“Œ Home":

    st.title("ğŸ“Š Web3 Chain Radar Dashboard")

    # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    fg = load_fear_greed_api()
    global_mkt = load_global_market()

    # ì‹¤ì‹œê°„ ê°€ê²©
    coin_list = [
        {"id": "bitcoin", "symbol": "BTC"},
        {"id": "ethereum", "symbol": "ETH"},
        {"id": "solana", "symbol": "SOL"},
    ]

    prices = load_prices_multi(coin_list)

    col1, col2, col3, col4 = st.columns(4)
   
    with col1:
        fear_greed_card(fg["score"], fg["diff"])

    with col2:
        colored_metric("BTC", prices["BTC"]["price"], prices["BTC"]["change"])

    with col3:
        colored_metric("ETH", prices["ETH"]["price"], prices["ETH"]["change"])

    with col4:
        colored_metric("SOL", prices["SOL"]["price"], prices["SOL"]["change"])

# ===============================================
# Global Market Summary (ì‹¤ì‹œê°„)
# ===============================================
    
    st.subheader("ğŸŒ Global Market Summary")

    gm = load_global_market()

    g1, g2, g3, g4 = st.columns(4)

    with g1:
        custom_metric(
            "ì „ì²´ ì•”í˜¸í™”í ì‹œì´ (USD)",
            f"{gm['market_cap']:,.0f}",
            gm["market_cap_change_24h"]
        )

    with g2:
        custom_metric(
            "24h ê±°ë˜ëŸ‰ (USD)",
            f"{gm['volume_24h']:,.0f}"
        )

    with g3:
        colored_status("BTC Dominance", f"{gm['btc_dominance']:.2f}%")

    with g4:
        colored_status("ETH Dominance", f"{gm['eth_dominance']:.2f}%")


    # ======== 3 COLUMN LAYOUT ========
    left, center, right = st.columns([2, 5, 2])

    # LEFT ---------------------------
    with left:
        st.subheader("ğŸ“Œ ì‹œì¥ ìš”ì•½ ë©”ëª¨")
        st.write(
            "- Fear & Greed ì§€ìˆ˜: ì‹œì¥ ì‹¬ë¦¬\n"
            "- BTC/ETH/SOL: ë‹¨ê¸° ê°€ê²© ëª¨ë‹ˆí„°ë§\n"
            "- ê¸€ë¡œë²Œ ì‹œì´ / Dominance: ìê¸ˆ íë¦„ ì²´í¬\n"
        )   

    # CENTER ---------------------------
    with center:
        st.subheader("ğŸ“ˆ BTC Active Addresses (30ì¼ ì‹¤ë°ì´í„°)")
        btc_active = load_btc_active_addresses()
        st.plotly_chart(
            px.line(btc_active, x="date", y="active_addresses", height=300),
            use_container_width=True
        )
      

    # RIGHT ---------------------------
    with right:
        st.subheader("ğŸ“‰ ë¦¬ìŠ¤í¬ ë¶„ì„")

        # Fear & Greed ê¸°ë°˜ ì‹œì¥ ë¦¬ìŠ¤í¬ ë“±ê¸‰
        score = fg["score"]
        risk = "ë†’ìŒ" if score > 70 else "ì¤‘ê°„" if score > 40 else "ë‚®ìŒ"

        colored_status("ì‹œì¥ ë¦¬ìŠ¤í¬", risk)


        # BTC ì¶”ì„¸ íŒë‹¨
        trend = "í™•ì¥ êµ­ë©´" if prices["BTC"]["change"] > 0 else "ì¶•ì†Œ êµ­ë©´"
        colored_status("BTC ì¶”ì„¸", trend)




# ===============================================
# PAGE 2 â€” NEWS (ê°ì„± ì œê±° + 10ê°œì”© í˜ì´ì§€)
# ===============================================
elif page == "ğŸ“° News":

    st.title("ğŸ“° Web3 ë‰´ìŠ¤ ë¶„ì„ (ê¸€ë¡œë²Œ + í•œêµ­ì–´)")

    df = load_news_all()

    if df.empty:
        st.warning("ë¶ˆëŸ¬ì˜¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
    else:
        # TextRank ìš”ì•½ + KeyBERT í‚¤ì›Œë“œ ìƒì„±
        df["summary"] = df["summary_raw"].apply(lambda x: textrank_summarize(x, max_sent=3))
        df["keywords"] = df["summary_raw"].apply(lambda x: extract_keywords(x, top_k=5))

        # ì–¸ì–´ í•„í„°
        st.subheader("ğŸ§© í•„í„°")
        lang_opt = st.selectbox("ì–¸ì–´", ["ì „ì²´", "í•œêµ­ì–´ë§Œ", "ì˜ì–´ë§Œ"])

        df_page_base = df.copy()

        if lang_opt == "í•œêµ­ì–´ë§Œ":
            df_page_base = df_page_base[df_page_base["lang"] == "ko"]
        elif lang_opt == "ì˜ì–´ë§Œ":
            df_page_base = df_page_base[df_page_base["lang"] == "en"]

        # -------- Pagination (10ê°œì”© ì¶œë ¥) --------
        page_size = 10
        total_pages = (len(df_page_base) - 1) // page_size + 1

        current_page = st.number_input(
            "í˜ì´ì§€ ì„ íƒ (10ê°œì”© í‘œì‹œ)",
            min_value=1,
            max_value=total_pages,
            step=1
        )

        start = (current_page - 1) * page_size
        end = start + page_size

        df_page = df_page_base.iloc[start:end]   # â† ì—¬ê¸°! df_view â†’ df_page

        # í…Œì´ë¸” ìš”ì•½
        st.subheader("ğŸ“„ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸")
        st.dataframe(
            df_page[["title", "source", "lang", "keywords"]],
            height=300
        )

        # ë‰´ìŠ¤ ì¹´ë“œ ìƒì„¸
        st.subheader("ğŸ“° ë‰´ìŠ¤ ìƒì„¸ ì¹´ë“œ")

        for _, row in df_page.iterrows():        # â† ì—¬ê¸°! df_view â†’ df_page
            st.markdown(f"### {row['title']}")
            st.markdown(f"**Source:** {row['source']} Â· **ì–¸ì–´:** {row['lang']}")
            st.markdown(f"**í‚¤ì›Œë“œ:** {row['keywords']}")
            st.write(row["summary"])
            st.divider()

        # WordCloud (ìš”ì•½ ê¸°ë°˜)
        st.subheader("â˜ï¸ ìš”ì•½ ê¸°ë°˜ WordCloud")
        text_wc = " ".join(df_page["summary"].tolist())

        fig_wc = generate_wordcloud(text_wc)
        st.pyplot(fig_wc)



# ===============================================
# PAGE 3 â€” SECTORS
# ===============================================
elif page == "ğŸ§© Sectors":

    st.title("ğŸ§© Web3 ì„¹í„° ë¶„ì„ â€” í•µì‹¬ 6ê°œ ê·¸ë£¹")

    sectors_rt = load_sectors_realtime()
    if sectors_rt.empty:
        st.warning("ì„¹í„° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        # í•µì‹¬ ì„¹í„°ë³„ ì‹œì´/ë³€í™”ìœ¨ ì§‘ê³„
        core_summary = (
            sectors_rt
            .groupby("core_sector")
            .agg(
                total_mcap=("market_cap", "sum"),
                avg_mcap_chg=("market_cap_change_24h", "mean")
            )
            .reset_index()
        )

        # Infra/ê¸°íƒ€ëŠ” ë§¨ ì•„ë˜ë¡œ ë³´ë‚´ê¸°
        core_summary["sort_key"] = core_summary["core_sector"].apply(
            lambda x: 1 if x == "Infra/ê¸°íƒ€" else 0
        )
        core_summary = core_summary.sort_values(["sort_key", "core_sector"]).drop(columns=["sort_key"])

        st.subheader("ğŸ“Š í•µì‹¬ ì„¹í„°ë³„ ì‹œì´ & 24h ë³€í™”ìœ¨")
        st.dataframe(core_summary, height=300)

        # ë³€í™”ìœ¨ ë°” ì°¨íŠ¸
        st.subheader("ğŸ“ˆ ì„¹í„°ë³„ 24h ì‹œì´ ë³€í™”ìœ¨ (í‰ê· )")
        fig_bar = px.bar(
            core_summary,
            x="core_sector",
            y="avg_mcap_chg",
            labels={"core_sector": "ì„¹í„°", "avg_mcap_chg": "24h ë³€í™”ìœ¨(%)"},
        )
        st.plotly_chart(fig_bar, use_container_width=True)

        st.subheader("ğŸ“ˆ ì„¹í„°ë³„ Top Movers (ì½”ì¸ ë‹¨ìœ„)")

        core_choices = core_summary["core_sector"].tolist()
        chosen_core = st.selectbox("ë¶„ì„í•  ì„¹í„° ì„ íƒ", core_choices)

        # ì„ íƒëœ core ì„¹í„°ì— ì†í•œ ì›ì‹œ ì¹´í…Œê³ ë¦¬ë“¤
        subset_cats = sectors_rt[sectors_rt["core_sector"] == chosen_core]

        all_gainers = []
        all_losers = []

        for _, row in subset_cats.iterrows():
            cat_id = row["category_id"]
            top_g, top_l = load_sector_top_movers(cat_id, top=5)
            if not top_g.empty:
                top_g["category"] = row["sector"]
                all_gainers.append(top_g)
            if not top_l.empty:
                top_l["category"] = row["sector"]
                all_losers.append(top_l)

        if all_gainers:
            df_g = pd.concat(all_gainers, ignore_index=True)
            df_g = df_g.sort_values("price_change_percentage_24h", ascending=False).head(10)
            st.markdown("ğŸ”¼ **ìƒìŠ¹ Top 10 ì½”ì¸**")
            st.dataframe(df_g, height=300)
        else:
            st.info("ìƒìŠ¹ ì½”ì¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        if all_losers:
            df_l = pd.concat(all_losers, ignore_index=True)
            df_l = df_l.sort_values("price_change_percentage_24h", ascending=True).head(10)
            st.markdown("ğŸ”½ **í•˜ë½ Top 10 ì½”ì¸**")
            st.dataframe(df_l, height=300)
        else:
            st.info("í•˜ë½ ì½”ì¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
